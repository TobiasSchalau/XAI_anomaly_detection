{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "The Jupyter notebook consists of three parts: \n",
    "\n",
    "1. Preprocessing of the NSL-KDD data set\n",
    "2. Train of a fully connected DNN\n",
    "3. Execution of the XAI methods for getting explanations for the model\n",
    "\n",
    "The code for executing these steps is not part of the notebook. Instead each step is done in a separated class written in python. The Juptyter notebook acts like a 'main.py' for executing the different steps of the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load module\n",
    "from xai_anomaly_detection.preprocessing import preprocessing\n",
    "\n",
    "# Initialise instance which loads the data\n",
    "Preprocessing = preprocessing.PreprocessNSLKDD()\n",
    "# show initial shape and first 2 lines of data set\n",
    "Preprocessing.print_overview('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start preprocessing step\n",
    "# one-hot encoding of categorical features\n",
    "# min-max normalization \n",
    "# convert all sub attack classes to common 'attack' label\n",
    "Preprocessing.preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape and first 2 lines\n",
    "Preprocessing.print_overview('train')\n",
    "\n",
    "# The paper said after preprocessing there will be 122 features\n",
    "# but I get 124 features (with the label column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train data separated in features and labels\n",
    "(x_train, y_train) = Preprocessing.get_data()\n",
    "\n",
    "print(\"Shape y: \", y_train.shape)\n",
    "print(\"Shape X: \", x_train.shape)\n",
    "\n",
    "# columns of features\n",
    "columns = Preprocessing.test_data.columns[Preprocessing.test_data.columns != 'outcome']\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model initialization and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from xai_anomaly_detection.model.FCModel import FCModel, f1_m, precision_m, recall_m\n",
    "\n",
    "# initialise subclasses tf model\n",
    "model = FCModel(x_train.shape[1])\n",
    "# compile model\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "    metrics = ['accuracy', precision_m, recall_m, f1_m]\n",
    ")\n",
    "model.build(x_train.shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model if not exists\n",
    "from os.path import exists\n",
    "if exists('tmp/weights.index'):\n",
    "    model.load_weights('tmp/weights')\n",
    "else:\n",
    "    model.fit(x_train, y_train, epochs=5, batch_size=64)\n",
    "    model.save_weights('tmp/weights', save_format='tf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "(x_test, y_test) = Preprocessing.get_data(test_data=True)\n",
    "scores = model.evaluate(x_test, y_test)\n",
    "for i in range(1, len(model.metrics_names)):\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[i], scores[i]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build another model for SHAP\n",
    "Reason: see below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xai_anomaly_detection.model.FCModel import get_sequential_model\n",
    "\n",
    "# A bug causing 'model.outputs' to be 'None' for subclassed models\n",
    "# see https://github.com/tensorflow/tensorflow/issues/45202\n",
    "# this forces me to create another model\n",
    "\n",
    "# get compiled model\n",
    "seq_model = get_sequential_model(x_train.shape[1])\n",
    "\n",
    "# train model\n",
    "if exists('tmp/seq_model_weights.index'):\n",
    "    seq_model.load_weights('tmp/seq_model_weights')\n",
    "else:\n",
    "    seq_model.fit(x_train, y_train, epochs=5, batch_size=64)\n",
    "    seq_model.save_weights('tmp/seq_model_weights', save_format='tf')\n",
    "\n",
    "# evaluate\n",
    "scores = seq_model.evaluate(x_test, y_test)\n",
    "for i in range(1, len(seq_model.metrics_names)):\n",
    "    print(\"%s: %.2f%%\" % (seq_model.metrics_names[i], scores[i]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xai_anomaly_detection.explanations.shap import shap_explanations\n",
    "# initialise shap class and create explainer for model\n",
    "Shap = shap_explanations(seq_model, x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate global explanation with SHAP summary plot\n",
    "Shap.generate_summary_plot(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local explanation with a SHAP force plot\n",
    "Shap.generate_force_plot(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local explanation for multiple samples\n",
    "# Shap.generate_collective_force_plot(columns, x_test)\n",
    "# https://github.com/slundberg/shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local explanations with LIME\n",
    "\n",
    "# select random sample\n",
    "import numpy as np\n",
    "x_rand = x_test[np.random.randint(x_test.shape[0], size=1)].flatten()\n",
    "\n",
    "from xai_anomaly_detection.explanations.lime import lime_explanations\n",
    "Lime = lime_explanations(x_train, columns)\n",
    "\n",
    "# note: graph background is transparent \n",
    "# thus it is a little bit ugly in dark mode\n",
    "\n",
    "# here I used the original model instead of sequential model\n",
    "# it proofs that the model is correctly build and only the bug \n",
    "# in tf prevents to execute shap on it\n",
    "Lime.generate_lime_explanation(model, x_rand, num_features=10, show_table=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
