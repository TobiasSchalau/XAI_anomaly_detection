{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "The Jupyter notebook consists of three parts: \n",
    "\n",
    "1. Preprocessing of the NSL-KDD data set\n",
    "2. Train of a fully connected DNN\n",
    "3. Execution of the XAI methods for getting explanations for the model\n",
    "\n",
    "The code for executing these steps is not part of the notebook. Instead each step is done in a separated class written in python. The Juptyter notebook acts like a 'main.py' for executing the different steps of the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common dependencies\n",
    "from os.path import exists\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load own modules\n",
    "from xai_anomaly_detection.explanations import protodash\n",
    "from xai_anomaly_detection.explanations import brcg\n",
    "from xai_anomaly_detection.explanations.shap import shap_explanations\n",
    "from xai_anomaly_detection.explanations.lime import lime_explanations\n",
    "from xai_anomaly_detection.preprocessing import preprocessing\n",
    "from xai_anomaly_detection.model.FCModel import FCModel, f1_m, precision_m, recall_m, get_sequential_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise instance which loads the data\n",
    "Preprocessing = preprocessing.PreprocessNSLKDD()\n",
    "# show head of train data set\n",
    "display(Preprocessing.train_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start preprocessing step\n",
    "# one-hot encoding of categorical features\n",
    "# min-max normalization \n",
    "# convert all sub attack classes to common 'attack' label\n",
    "Preprocessing.preprocessing()\n",
    "\n",
    "# show head of train data set after preprocessing\n",
    "display(Preprocessing.train_data.head(5))\n",
    "\n",
    "# The paper said after preprocessing there will be 122 features\n",
    "# but I get 124 features (with the label column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train data separated in features and labels\n",
    "(x_train, y_train) = Preprocessing.get_data()\n",
    "\n",
    "print(\"Shape y: \", y_train.shape)\n",
    "print(\"Shape x: \", x_train.shape)\n",
    "\n",
    "# columns of features\n",
    "columns = Preprocessing.test_data.columns[Preprocessing.test_data.columns != 'outcome']\n",
    "display(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model initialization and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise subclasses tf model\n",
    "model = FCModel(x_train.shape[1])\n",
    "# compile model\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "    metrics = ['accuracy', precision_m, recall_m, f1_m]\n",
    ")\n",
    "model.build(x_train.shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model if not exists\n",
    "if exists('tmp/weights.index'):\n",
    "    model.load_weights('tmp/weights')\n",
    "else:\n",
    "    model.fit(x_train, y_train, epochs=5, batch_size=64)\n",
    "    model.save_weights('tmp/weights', save_format='tf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "(x_test, y_test) = Preprocessing.get_data(test_data=True)\n",
    "scores = model.evaluate(x_test, y_test)\n",
    "for i in range(1, len(model.metrics_names)):\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[i], scores[i]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build another model for SHAP\n",
    "Reason: see below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bug causing 'model.outputs' to be 'None' for subclassed models\n",
    "# see https://github.com/tensorflow/tensorflow/issues/45202\n",
    "# this forces me to create another model\n",
    "\n",
    "# get compiled model\n",
    "seq_model = get_sequential_model(x_train.shape[1])\n",
    "\n",
    "# train model\n",
    "if exists('tmp/seq_model_weights.index'):\n",
    "    seq_model.load_weights('tmp/seq_model_weights')\n",
    "else:\n",
    "    seq_model.fit(x_train, y_train, epochs=5, batch_size=64)\n",
    "    seq_model.save_weights('tmp/seq_model_weights', save_format='tf')\n",
    "\n",
    "# evaluate\n",
    "scores = seq_model.evaluate(x_test, y_test)\n",
    "for i in range(1, len(seq_model.metrics_names)):\n",
    "    print(\"%s: %.2f%%\" % (seq_model.metrics_names[i], scores[i]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise shap class and create explainer for model\n",
    "Shap = shap_explanations(seq_model, x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate global explanation with SHAP summary plot\n",
    "Shap.generate_summary_plot(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local explanation with a SHAP force plot\n",
    "Shap.generate_force_plot(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local explanation for multiple samples\n",
    "# Shap.generate_collective_force_plot(columns, x_test)\n",
    "# https://github.com/slundberg/shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local explanations with LIME\n",
    "\n",
    "# select random sample\n",
    "x_rand = x_test[np.random.randint(x_test.shape[0], size=1)].flatten()\n",
    "\n",
    "Lime = lime_explanations(x_train, columns)\n",
    "\n",
    "# note: graph background is transparent \n",
    "# thus it is a little bit ugly in dark mode\n",
    "\n",
    "# here I used the original model instead of sequential model\n",
    "# it proofs that the model is correctly build and only the bug \n",
    "# in tf prevents to execute shap on it\n",
    "Lime.generate_lime_explanation(model, x_rand, num_features=10, show_table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BRCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brcg needs dataframes as input\n",
    "(x_train_df, y_train_df) = Preprocessing.get_data(test_data=False, as_df=True)\n",
    "\n",
    "# after 12 minutes I stopped the training and decided to train with a smaller set\n",
    "# with 0.01 of the data set it still took > 4 min\n",
    "indices = np.random.choice(x_train_df.shape[0], replace = False, size=int(0.001*x_train_df.shape[0]))\n",
    "x_train_df = x_train_df.iloc[indices]\n",
    "y_train_df = y_train_df.iloc[indices]\n",
    "\n",
    "print(x_train_df.shape[0])\n",
    "print(y_train_df.shape[0])\n",
    "\n",
    "# generate and print BRCG rules\n",
    "brcg.explain_rules(x_train_df, y_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "data_type = {'age': np.float,\n",
    "             'workclass': str,\n",
    "             'fnlwgt': np.float,\n",
    "             'education': str,\n",
    "             'education-num': np.float,\n",
    "             'marital-status': str,\n",
    "             'occupation': str,\n",
    "             'relationship': str,\n",
    "             'race': str,\n",
    "             'sex': str,\n",
    "             'capital-gain': np.float,\n",
    "             'capital-loss': np.float,\n",
    "             'native-country': str,\n",
    "             'hours-per-week': np.float,\n",
    "             'label': str}\n",
    "\n",
    "col_names = ['age', 'workclass', 'fnlwgt', 'education',\n",
    "             'education-num', 'marital-status', 'occupation',\n",
    "             'relationship', 'race', 'sex',\n",
    "             'capital-gain', 'capital-loss', 'hours-per-week',\n",
    "             'native-country', 'label']\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data',\n",
    "                 header=None,\n",
    "                 delimiter=', ',\n",
    "                 engine='python',\n",
    "                 names=col_names,\n",
    "                 dtype=data_type)\n",
    "df.columns = df.columns.str.replace('-', '_')\n",
    "TARGET_COLUMN = 'label'\n",
    "\n",
    "\n",
    "POS_VALUE = '>50K' # Setting positive value of the label for which we train\n",
    "values_dist = df[TARGET_COLUMN].value_counts()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "# Split the data set into 80% training and 20% test set\n",
    "print('Training set:')\n",
    "print(train[TARGET_COLUMN].value_counts())\n",
    "print('Test set:')\n",
    "print(test[TARGET_COLUMN].value_counts())\n",
    "\n",
    "y_train = train[TARGET_COLUMN].apply(lambda x: 1 if x == POS_VALUE else 0)\n",
    "x_train = train.drop(columns=[TARGET_COLUMN])\n",
    "\n",
    "y_test = test[TARGET_COLUMN].apply(lambda x: 1 if x == POS_VALUE else 0)\n",
    "x_test = test.drop(columns=[TARGET_COLUMN])\n",
    "\n",
    "\n",
    "# Split data frames into features and label\n",
    "import time\n",
    "from aix360.algorithms.rule_induction.rbm.boolean_rule_cg import BooleanRuleCG as BRCG\n",
    "from aix360.algorithms.rbm import FeatureBinarizer\n",
    "fb = FeatureBinarizer(negations=True)\n",
    "X_train_fb = fb.fit_transform(x_train)\n",
    "x_test_fb = fb.transform(x_test)\n",
    "\n",
    "explainer = BRCG(silent=True)\n",
    "start_time = time.time()\n",
    "explainer.fit(X_train_fb, y_train)\n",
    "end_time = time.time()\n",
    "print('Training time (sec): ' + str(end_time - start_time))\n",
    "\n",
    "# compute performance metrics on test set\n",
    "y_pred = explainer.predict(x_test_fb)\n",
    "\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Balanced accuracy:', balanced_accuracy_score(y_test, y_pred))\n",
    "print('Precision:', precision_score(y_test, y_pred, pos_label=1))\n",
    "print('Recall:', recall_score(y_test, y_pred, pos_label=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ProtoDash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanations with ProtoDash form data\n",
    "(x_train_df, y_train_df) = Preprocessing.get_data(test_data=False, as_df=True)\n",
    "indices = np.random.choice(x_train_df.shape[0], replace = False, size=int(0.01*x_train_df.shape[0]))\n",
    "x_train_df = x_train_df.iloc[indices]\n",
    "y_train_df = y_train_df.iloc[indices]\n",
    "# for full data set I get:\n",
    "# MemoryError: Unable to allocate 17.4 GiB for an array with shape (125972, 18488) and data type float64\n",
    "# such I also use a smaller data set\n",
    "\n",
    "# generate protodash explanations from data\n",
    "display(protodash.generate_protodash_explanations(x_train_df))\n",
    "\n",
    "# sometimes the generation crashes with error:\n",
    "# TypeError: bad operand type for unary -: 'NoneType'\n",
    "# I couldn't find it\n",
    "# maybe a bug? https://githubhelp.com/Trusted-AI/AIX360/issues/75"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
